---
title: 'Multi-scale Motion Contrastive Learning for Self-supervised Skeleton-based Action Recognition'
author: Yushan Wu, Zengmin Xu, Mengwei Yuan, Tianchi Tang, Ruxing Meng, Zhongyuan Wang
publishDate: 2024-08-30
publishDesc: 'Multimedia Systems'
description: ''
doi: 10.1007/s00530-024-01463-0
cite: 'Wu Y, Xu Z, Yuan M, et al. Multi-scale motion contrastive learning for self-supervised skeleton-based action recognition[J]. Multimedia Systems, 2024, 30(5): 1-14.'
additionalInfo: SCI, CCF C
links:
  - label: 'PDF'
    url: '/pdfs/[2024] Multi-scale Motion Contrastive Learning for Self-supervised Skeleton-based Action Recognition.pdf'
  - label: 'Code'
    url: 'https://github.com/zengminxu/MsMCLR-Code'
thumbnail: '/thumbnails/2024-multi-scale.png'
---

## Abstract

People process things and express feelings through actions, action recognition has been able to be widely studied, yet under-explored. Traditional self-supervised skeleton-based action recognition focus on joint point features, ignoring the inherent semantic information of body structures at different scales. To address this problem, we propose a multi-scale Motion Contrastive Learning of Visual Representations (MsMCLR) model. The model utilizes the Multi-scale Motion Attention (MsM Attention) module to divide the skeletal features into three scale levels, extracting cross-frame and cross-node motion features from them. To obtain more motion patterns, a combination of strong data augmentation is used in the proposed model, which motivates the model to utilize more motion features. However, the feature sequences generated by strong data augmentation make it difficult to maintain identity of the original sequence. Hence, we introduce a dual distributional divergence minimization method, proposing a multi-scale motion loss function. It utilizes the embedding distribution of the ordinary augmentation branch to supervise the loss computation of the strong augmentation branch. Finally, the proposed method is evaluated on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets. The accuracy of our method is 1.4 âˆ¼ 3.0% higher than the frontier models.
